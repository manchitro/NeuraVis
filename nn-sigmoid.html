<!DOCTYPE html>
<html lang="en">
	<head>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-0LCX0TLTK4"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag() {
				dataLayer.push(arguments);
			}
			gtag("js", new Date());

			gtag("config", "G-0LCX0TLTK4");
		</script>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<title>Sigmoid Neurons</title>
		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap" rel="stylesheet" />
		<link rel="stylesheet" href="assets/css/style.css" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous" />
		<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
		<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
		<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" integrity="sha384-w1Q4orYjBQndcko6MimVbzY0tgp4pWB4lZ7lr30WKz0vr/aWKhXdBNmNb5D92v7s" crossorigin="anonymous"></script>
		<script src="./assets/js/include.js"></script>
		<link rel="icon" type="image/png" href="assets/images/favicon.png" />
	</head>
	<body>
		<script>
			include("./partials/nav.html", document.currentScript);
		</script>
		<script src="./assets/js/active.js"></script>
		<!-- <div class="ud d-flex justify-content-center bg-primary py-2 text-light">Page under development</div> -->
		<div role="main" class="container-sm my-4 border-dark border-right border-left">
			<h1 class="mb-2">Sigmoid Neurons</h1>
			<p class="">
				Previously, we learnt about perceptrons and how they take binary inputs and produce a binary output. <br />
				Sigmoid neurons are almost identical to perceptrons. The only difference is that, sigmoids take any floating point number between 0 and 1 as inputs and outputs the same.
			</p>
			<img src="./assets/images/sigmoid-example-01.jpg" alt="Sigmoid Neuron Example" class="mw-100 h-auto" />
			<p>A sigmoid's input and output might look something like the above image. See that it is no longer constrained as binary but any floating point number between 0 and 1.</p>
			<p>This tiny change results in some incredible behavioural upgrades to our Neural Network. We are no longer stuck with limited inputs and outputs. We can now fine tune our network to cause small changes in output in the event of small changes in inputs.</p>
			<p>You might remember from our discussion of <a href="nn-perceptron.html">Perceptrons</a> that the output is calculated with the expression:</p>
			<div class="math w-75 mx-auto mb-3">∑<sub>j</sub>w<sub>j</sub>x<sub>j</sub> + b</div>

			<p>We can make some notational changes to make it easier for us to write the weighted sum as a dot product:</p>
			<div class="math w-75 mx-auto mb-3">∑<sub>j</sub>w<sub>j</sub>x<sub>j</sub> + b = w . x + b</div>
			<p>where, <strong>w</strong> and <strong>x</strong> are vectors whose elements are weights and inputs respectively. <strong>w</strong> and <strong>x</strong> look like this:</p>
			<div class="math w-75 mx-auto mb-3">
				w = [w<sub>1</sub>, w<sub>2</sub> ... w<sub>j</sub>] <br />
				x = [x<sub>1</sub>, x<sub>2</sub> ... x<sub>j</sub>]
			</div>
			<p>Their dot product will be:</p>
			<div class="math w-75 mx-auto mb-3">(x<sub>1</sub>*w<sub>1</sub>) + (x<sub>2</sub>*w<sub>2</sub>) + (x<sub>3</sub>*w<sub>3</sub>)</div>
			<p>which ultimately is our original weighted sum. Therefore:</p>
			<div class="math w-75 mx-auto mb-3">∑<sub>j</sub>w<sub>j</sub>x<sub>j</sub> = w . x</div>
			<p>Now, there is no gurantee that this output of a neuron will be a number between 0 and 1. To make sure that it is, we put the output in a special function called a sigmoid function. Hence, the name. The function looks like this:</p>
			<div class="math w-75 mx-auto mb-3">
				S(x) = 1/(1+e<sup>-z</sup>)
			</div>
			<p>All this function does is squeeze the output into a value between 0 and 1. The algebraic form a sigmoid function looks like this:</p>
			<img src="./assets/images/sigmoid.png" alt="Sigmoid Neuron Example" class="mw-100 h-auto" />
			<p>From the graph we can see that the output of the sigmoid function is always between 0 and 1. We can also see that for negative numbers the result tend to be closer to 0 and vise versa. Inputting 0 will always result in 0.5.<br>So if we put the output of our sigmoid neuron into the function, the output will be:</p>
			<div class="math w-75 mx-auto mb-3">
				1/(1+exp(-w.x+b))
			</div>
			<p>It means that, for any small changes in the weights (Δw<sub>j</sub>) and the bias (Δb) will result in a small change in the output (Δoutput). You can try to plug different values in the sigmoid function and observe the output using a calculator (or by hand if you're good at math). Now that we know about sigmoid neurons, we can move on to making our neural network that can recognize handwritten digits.</p>
			<div class="page-nav d-flex flex-row justify-content-between">
				<a href="nn-perceptron.html">< Previous</a>
				<a href="nn-structure.html">Next ></a>
			</div>
		</div>
		<script>
			include("./partials/footer.html", document.currentScript);
		</script>
	</body>
</html>
